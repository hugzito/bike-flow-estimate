{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1303a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "import os, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "from torch_geometric.nn.models import Node2Vec\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "GCNConv._orig_propagate = GCNConv.propagate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.explain import GNNExplainer, Explainer\n",
    "from models import *\n",
    "from tg_functions import *\n",
    "from bike_functions import *\n",
    "\n",
    "dropout_p = 0.5\n",
    "use_gat = True\n",
    "bins = [int(i) for i in os.getenv(\"BINS\", \"400 800 1300 2100 3000 3700 4700 7020 9660\").split(' ')]\n",
    "# bins = [int(i) for i in os.getenv(\"BINS\", \"3000\").split(' ')]\n",
    "\n",
    "bins = torch.tensor(bins, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_c = 250\n",
    "num_layers = 0\n",
    "random_seed = 100\n",
    "nh = 1\n",
    "\n",
    "graph_num = 29  # Replace with your graph number\n",
    "\n",
    "model_name = 'upbeat-firebrand-246' # Replace with your model name\n",
    "\n",
    "weight_prefix = 'best_accuracy'  # Replace with your weight prefix\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\", flush = True)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\", flush = True)\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "with open(f'../data/graphs/{graph_num}/linegraph_tg.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data.edge_index = data.edge_index.contiguous()\n",
    "data.x = data.x.contiguous()\n",
    "data.y = data.y.contiguous()\n",
    "\n",
    "data = stratified_split(data = data , random_seed = random_seed)\n",
    "\n",
    "# --- Model Instantiation ---\n",
    "model = GAT(hidden_c, num_layers, random_seed, bins, data, nh).to(device) if use_gat else GCN(hidden_c, num_layers, random_seed, bins, data).to(device)\n",
    "\n",
    "if use_gat == 'MLP':\n",
    "    model = MLP(hidden_c, num_layers, random_seed, bins, data, nh).to(device)\n",
    "\n",
    "# Load the model with the GCN class\n",
    "model = torch.load(f'../data/graphs/{graph_num}/models/{model_name}.pt', map_location=device)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../data/graphs/{graph_num}/models/{model_name}_{weight_prefix}.pt', map_location=device))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "data.edge_index = data.edge_index.contiguous()\n",
    "data.x = data.x.contiguous()\n",
    "data.y = data.y.contiguous()\n",
    "print(data.x.shape, data.edge_index.shape, data.y.shape, flush = True)\n",
    "data = stratified_split(data, random_seed=random_seed)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ffd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, data, criterion, device, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import GNNExplainer, Explainer\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type=None,\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data.val_mask.squeeze() & (data.y > 0).squeeze()\n",
    "node_idx = torch.nonzero(mask, as_tuple=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data.x.to(device), data.edge_index.to(device))\n",
    "pred = out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_column = build_value_to_column_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume 'graph_num', 'node_idx', 'data', 'model', 'explainer' already exist\n",
    "\n",
    "# Load feature names\n",
    "feats_df = pd.DataFrame(columns=data.feat_names, data=data.x.cpu().detach().numpy())\n",
    "\n",
    "# Your secondary to primary mapping\n",
    "secondary_to_primary = {\n",
    "    # [your dictionary here]\n",
    "}\n",
    "\n",
    "# Initialize storage\n",
    "scores = {}       # Collect importance scores\n",
    "gradients = {}    # Collect target class gradients\n",
    "all_bin_grads_total = {}  # Collect all bin gradients\n",
    "\n",
    "data_x = data.x.to(device)\n",
    "edge_index = data.edge_index.to(device)\n",
    "data_y = data.y.to(device)\n",
    "\n",
    "data_x.requires_grad_(True)\n",
    "\n",
    "for idx in node_idx:\n",
    "    # 1. Explain the node\n",
    "    explanation = explainer(data_x, edge_index, index=idx)\n",
    "    node_mask = explanation.node_mask.squeeze()\n",
    "\n",
    "    # 2. Prediction and true label\n",
    "    curr_pred = pred[idx].item()\n",
    "    target = int(torch.bucketize(data_y[idx], bins.to(data_y.device)))\n",
    "\n",
    "    # 3. Importance scores (all features)\n",
    "    score = node_mask.sum(dim=0).detach().cpu().numpy().flatten()\n",
    "\n",
    "    # 4. Per-bin gradients\n",
    "    node_bin_grads = []\n",
    "    for bin_idx in range(model(data_x, edge_index).shape[1]):\n",
    "        model.zero_grad()\n",
    "        out = model(data_x, edge_index)\n",
    "        out[idx, bin_idx].backward(retain_graph=True)\n",
    "        grad = data_x.grad[idx].detach().cpu().numpy().flatten()\n",
    "        node_bin_grads.append(grad)\n",
    "        data_x.grad.zero_()\n",
    "    node_bin_grads = np.stack(node_bin_grads)  # (num_bins, num_features)\n",
    "\n",
    "    # 5. Save gradients for **all features**\n",
    "    for feat_idx in range(node_bin_grads.shape[1]):\n",
    "        if feat_idx not in all_bin_grads_total:\n",
    "            all_bin_grads_total[feat_idx] = []\n",
    "        all_bin_grads_total[feat_idx].append(node_bin_grads[:, feat_idx])\n",
    "\n",
    "        if feat_idx not in scores:\n",
    "            scores[feat_idx] = []\n",
    "            gradients[feat_idx] = []\n",
    "        \n",
    "        scores[feat_idx].append(score[feat_idx])\n",
    "        gradients[feat_idx].append(node_bin_grads[target, feat_idx])  # Use target class gradient\n",
    "\n",
    "# =====================================\n",
    "# Convert all_bin_grads_total into a clean DataFrame\n",
    "# =====================================\n",
    "\n",
    "# Build a dictionary ready for DataFrame\n",
    "feature_bin_gradient_dict = {}\n",
    "\n",
    "for feat_idx, grads_list in all_bin_grads_total.items():\n",
    "    feature_name = feats_df.columns[feat_idx]\n",
    "    \n",
    "    # Average across nodes per bin\n",
    "    grads_across_nodes = np.stack(grads_list)  # (num_nodes, num_bins)\n",
    "    avg_gradients = grads_across_nodes.mean(axis=0)  # (num_bins,)\n",
    "\n",
    "    feature_bin_gradient_dict[feature_name] = avg_gradients\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_bin_gradients_df = pd.DataFrame.from_dict(\n",
    "    feature_bin_gradient_dict, \n",
    "    orient='index', \n",
    "    columns=[f\"Bin_{i}\" for i in range(len(bins)+1)]\n",
    ")\n",
    "\n",
    "feature_bin_gradients_df.index.name = 'Feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df866c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# After all nodes are explained\n",
    "# ================================\n",
    "\n",
    "# Sort by feature index\n",
    "sorted_gradients = dict(sorted(gradients.items(), key=lambda item: item[0]))\n",
    "\n",
    "# Map feature index â†’ feature name\n",
    "sorted_gradients_named = {feats_df.columns[k]: v for k, v in sorted_gradients.items()}\n",
    "\n",
    "# Average importance score\n",
    "average_gradients = {feat: np.mean(values) for feat, values in sorted_gradients_named.items()}\n",
    "\n",
    "# =====================================\n",
    "# Fit real Linear Regression (gradients vs bin index)\n",
    "# =====================================\n",
    "\n",
    "feature_slopes = {}\n",
    "bin_indices = np.arange(len(bins)+1).reshape(-1, 1)  # 10 bins (0-9)\n",
    "\n",
    "for feat_idx, feat_name in enumerate(feats_df.columns):\n",
    "    if feat_idx not in all_bin_grads_total:\n",
    "        continue\n",
    "\n",
    "    # Stack across all explained nodes\n",
    "    grads_across_nodes = np.stack(all_bin_grads_total[feat_idx])  # (num_nodes, num_bins)\n",
    "    avg_grads_across_bins = np.mean(grads_across_nodes, axis=0)\n",
    "\n",
    "    reg = LinearRegression().fit(bin_indices, avg_grads_across_bins)\n",
    "    feature_slopes[feat_name] = reg.coef_[0]\n",
    "\n",
    "# =====================================\n",
    "# Build final DataFrame\n",
    "# =====================================\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Feature': list(average_gradients.keys()),\n",
    "    'Avg_Gradient': list(average_gradients.values()),\n",
    "    'Gradient_Trend_Slope': [feature_slopes.get(feat, 0.0) for feat in average_gradients.keys()]\n",
    "})\n",
    "\n",
    "summary_df['Abs_Avg_Gradient'] = summary_df['Avg_Gradient'].abs()\n",
    "\n",
    "# Group features based on secondary_to_primary\n",
    "def map_primary(feature_name):\n",
    "    return value_to_column.get(feature_name, feature_name)  # default to 'Other' if not found\n",
    "\n",
    "summary_df['Primary_Category'] = summary_df['Feature'].apply(map_primary)\n",
    "\n",
    "# # Sort by Primary Category then by Absolute Gradient\n",
    "# summary_df = summary_df.sort_values(['Primary_Category', 'Abs_Avg_Gradient'], ascending=[True, False])\n",
    "\n",
    "# # Save final output\n",
    "# summary_df.to_csv('feature_gradients_grouped_by_primary.csv', index=False)\n",
    "\n",
    "print(\"Saved grouped feature summary to 'feature_gradients_grouped_by_primary.csv'\")\n",
    "print(summary_df.head(15))  # Show first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cff411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Prepare the grouped summary_df\n",
    "summary_grouped = summary_df.groupby('Primary_Category').agg({\n",
    "    'Avg_Gradient': 'mean',\n",
    "    'Gradient_Trend_Slope': 'mean'\n",
    "}).sort_values('Avg_Gradient', ascending=False)\n",
    "\n",
    "primary_categories = list(summary_grouped.index)\n",
    "\n",
    "# Step 2: Build grouped feature Ã— bin matrix\n",
    "\n",
    "# Initialize matrix (Primary Categories x Bins)\n",
    "category_bin_gradients = np.zeros((len(primary_categories), len(bins) + 1))\n",
    "\n",
    "for idx, category in enumerate(primary_categories):\n",
    "    # Find all features belonging to this category\n",
    "    features_in_cat = summary_df[summary_df['Primary_Category'] == category]['Feature'].values\n",
    "\n",
    "    # For each feature, accumulate its bin gradients\n",
    "    grads_list = []\n",
    "    for feat_name in features_in_cat:\n",
    "        feat_idx_in_feats_df = list(feats_df.columns).index(feat_name)\n",
    "        if feat_idx_in_feats_df not in all_bin_grads_total:\n",
    "            continue\n",
    "\n",
    "        grads_across_nodes = np.stack(all_bin_grads_total[feat_idx_in_feats_df])  # (num_nodes x num_bins)\n",
    "        avg_grads_per_bin = np.mean(grads_across_nodes, axis=0)  # (num_bins,)\n",
    "        grads_list.append(avg_grads_per_bin)\n",
    "\n",
    "    # Average over all features in the category\n",
    "    if grads_list:\n",
    "        grads_list = np.stack(grads_list)\n",
    "        category_bin_gradients[idx, :] = grads_list.mean(axis=0)\n",
    "\n",
    "# Step 3: Compute Spearman correlation per category\n",
    "\n",
    "spearman_corrs = []\n",
    "\n",
    "for idx in range(category_bin_gradients.shape[0]):\n",
    "    grads = category_bin_gradients[idx, :]\n",
    "    bin_indices = np.arange(len(bins) + 1)\n",
    "\n",
    "    if np.all(grads == 0):\n",
    "        spearman_corr = 0.0\n",
    "    else:\n",
    "        spearman_corr, _ = spearmanr(bin_indices, grads)\n",
    "\n",
    "    spearman_corrs.append(spearman_corr)\n",
    "\n",
    "spearman_corrs = np.array(spearman_corrs)\n",
    "\n",
    "order = np.argsort(spearman_corrs)[::-1]\n",
    "primary_categories = np.array(primary_categories)[order]\n",
    "category_bin_gradients = category_bin_gradients[order, :]\n",
    "spearman_corrs = spearman_corrs[order]\n",
    "\n",
    "# Step 4: Plot side-by-side\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, len(primary_categories) * 0.2))\n",
    "\n",
    "xtick_labels = [f'< {int(i)}' for i in bins.tolist()] \n",
    "xtick_labels.append(f'>= {bins[-1]}')\n",
    "\n",
    "# Left: Gradients heatmap\n",
    "sns.heatmap(\n",
    "    category_bin_gradients,\n",
    "    ax=axes[0],\n",
    "    xticklabels=xtick_labels,\n",
    "    yticklabels=primary_categories,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"label\": \"Gradient Value\"},\n",
    "    vmin=-0.5,\n",
    "    vmax=0.5,\n",
    "    # annot=True,\n",
    ")\n",
    "axes[0].set_title(\"Average Gradient per Primary Category Across Bins\", fontsize=16)\n",
    "axes[0].set_xlabel(\"Bin\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Primary Category\", fontsize=14)\n",
    "\n",
    "# Right: Spearman correlation heatmap\n",
    "sns.heatmap(\n",
    "    spearman_corrs.reshape(-1, 1),\n",
    "    ax=axes[1],\n",
    "    yticklabels=primary_categories,\n",
    "    xticklabels=[\"Spearman Correlation\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    linewidths=0.2,\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    # annot=True,\n",
    ")\n",
    "axes[1].set_title(\"Spearman Correlation (Category Gradients vs Bin)\", fontsize=16)\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3babe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Step 1: Prepare features and y\n",
    "\n",
    "# Load feature data\n",
    "X = pd.DataFrame(data.x.cpu().detach().numpy(), columns=data.feat_names)\n",
    "\n",
    "y = data.y.cpu().numpy()  # Your original target values (unbinned)\n",
    "\n",
    "# Step 2: Mask X and y where y > 0\n",
    "mask = y > 0\n",
    "X_filtered = X.iloc[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Step 3: Compute Spearman correlation for each feature vs. y_filtered\n",
    "base_spearman_corrs = {}\n",
    "\n",
    "for feat in X_filtered.columns:\n",
    "    feature_values = X_filtered[feat].values\n",
    "\n",
    "    if np.all(feature_values == feature_values[0]):\n",
    "        corr = 0.0\n",
    "    else:\n",
    "        corr, _ = spearmanr(feature_values, y_filtered)\n",
    "\n",
    "    base_spearman_corrs[feat] = corr\n",
    "\n",
    "# Step 4: Convert to DataFrame\n",
    "base_corr_df = pd.DataFrame({\n",
    "    'Feature': list(base_spearman_corrs.keys()),\n",
    "    'Spearman_Feature_vs_Target': list(base_spearman_corrs.values())\n",
    "})\n",
    "\n",
    "# Step 5: Merge in Primary Category information\n",
    "\n",
    "# Assume you already have `summary_df` with Feature and Primary_Category\n",
    "base_corr_df = base_corr_df.merge(summary_df[['Feature', 'Primary_Category']], on='Feature', how='left')\n",
    "\n",
    "# Step 6: Group by Primary Category\n",
    "base_corr_grouped = base_corr_df.groupby('Primary_Category').agg({\n",
    "    'Spearman_Feature_vs_Target': 'mean'\n",
    "}).sort_values('Spearman_Feature_vs_Target', ascending=False)\n",
    "\n",
    "base_corr_grouped['Abs_Spearman'] = base_corr_grouped['Spearman_Feature_vs_Target'].abs()\n",
    "\n",
    "# Step 7: Plot\n",
    "\n",
    "plt.figure(figsize=(7, 5))  # Scale by number of groups\n",
    "\n",
    "sns.barplot(\n",
    "    data=base_corr_grouped.reset_index(),\n",
    "    y='Primary_Category',\n",
    "    x='Spearman_Feature_vs_Target',\n",
    "    palette='coolwarm_r',\n",
    ")\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.title('Base Spearman Correlation (Primary Category vs Target)', fontsize=16)\n",
    "plt.xlabel('Mean Spearman Correlation', fontsize=14)\n",
    "plt.ylabel('Primary Category', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab35bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_ranking(scores_dict, feats_df, top_k=None, as_dataframe=True):\n",
    "    \"\"\"\n",
    "    Maps feature indices to names and aggregates their importance scores.\n",
    "\n",
    "    Args:\n",
    "        scores_dict (dict): Dictionary of {feature_index: [importance scores]}.\n",
    "        feats_df (pd.DataFrame): DataFrame with original features (columns are feature names).\n",
    "        top_k (int or None): If set, only return top-k most important features.\n",
    "        as_dataframe (bool): If True, returns a sorted DataFrame. If False, returns a dict.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or dict: Ranked feature importance summary.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Aggregate scores (e.g., mean importance per feature)\n",
    "    summary = {\n",
    "        feats_df.columns[k]: np.mean(v)\n",
    "        for k, v in scores_dict.items()\n",
    "        if k < len(feats_df.columns)\n",
    "    }\n",
    "\n",
    "    # Sort descending by importance\n",
    "    sorted_items = sorted(summary.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    if top_k:\n",
    "        sorted_items = sorted_items[:top_k]\n",
    "\n",
    "    if as_dataframe:\n",
    "        return pd.DataFrame(sorted_items, columns=['Feature', 'Avg_Importance'])\n",
    "    else:\n",
    "        return dict(sorted_items)\n",
    "\n",
    "ranking_df = get_feature_importance_ranking(scores, feats_df, top_k=100)\n",
    "# display(ranking_df)\n",
    "\n",
    "plt.figure(figsize=(7, 5))  # Scale by number of groups\n",
    "\n",
    "sns.barplot(\n",
    "    data=ranking_df,\n",
    "    y='Feature',\n",
    "    x='Avg_Importance',\n",
    "    palette='coolwarm_r',\n",
    ")\n",
    "\n",
    "# plt.axvline(0, color='black', linestyle='--')\n",
    "plt.title('Feature Importance Ranking', fontsize=16)\n",
    "plt.xlabel('Mean Importance Score', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df.to_latex('feature_importance_ranking.tex', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df.to_csv(f'../data/graphs/{graph_num}/{model_name}_feature_importance_ranking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def collect_regression_feature_gradients(graph_num, data, node_idx, model, explainer, pred, device):\n",
    "    # Load feature names\n",
    "\n",
    "    # Initialize storage\n",
    "    scores = {}       # Importance scores from GNNExplainer\n",
    "    gradients = {}    # Gradients w.r.t. prediction (single target)\n",
    "    all_node_grads_total = {}  # Raw gradients across all nodes (per feature)\n",
    "\n",
    "    # Move data to device\n",
    "    data_x = data.x.to(device)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    data_y = data.y.to(device)\n",
    "\n",
    "    # Enable gradient tracking\n",
    "    data_x.requires_grad_(True)\n",
    "\n",
    "    for idx in node_idx:\n",
    "        # 1. Explain the node\n",
    "        explanation = explainer(data_x, edge_index, index=idx)\n",
    "        node_mask = explanation.node_mask.squeeze()\n",
    "\n",
    "        # 2. Predict\n",
    "        out = model(data_x, edge_index)\n",
    "        target_val = out[idx]\n",
    "\n",
    "        # 3. Backprop for gradient wrt output\n",
    "        model.zero_grad()\n",
    "        target_val.backward(retain_graph=True)\n",
    "        grad = data_x.grad[idx].detach().cpu().numpy().flatten()\n",
    "        data_x.grad.zero_()\n",
    "\n",
    "        # 4. Store gradients and explainer scores\n",
    "        for feat_idx in range(data_x.shape[1]):\n",
    "            if feat_idx not in scores:\n",
    "                scores[feat_idx] = []\n",
    "                gradients[feat_idx] = []\n",
    "                all_node_grads_total[feat_idx] = []\n",
    "\n",
    "            # Store GNNExplainer importance\n",
    "            scores[feat_idx].append(node_mask[feat_idx].item())\n",
    "\n",
    "            # Store raw gradient value\n",
    "            gradients[feat_idx].append(grad[feat_idx])\n",
    "            all_node_grads_total[feat_idx].append(grad[feat_idx])  # For averaging later\n",
    "\n",
    "    # Build DataFrame for per-feature average gradient\n",
    "    feature_grad_dict = {}\n",
    "    for feat_idx, grad_values in all_node_grads_total.items():\n",
    "        feature_name = data.feat_names[feat_idx]\n",
    "        avg_grad = np.mean(grad_values)\n",
    "        feature_grad_dict[feature_name] = avg_grad\n",
    "\n",
    "    feature_gradients_df = pd.DataFrame.from_dict(\n",
    "        feature_grad_dict, orient='index', columns=['Avg_Gradient']\n",
    "    )\n",
    "    feature_gradients_df.index.name = 'Feature'\n",
    "\n",
    "    return feature_gradients_df, scores, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344545a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
