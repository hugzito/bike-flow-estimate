{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "from torch_geometric.nn.models import Node2Vec\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "epochs = int(os.getenv(\"EPOCHS\", 1000))  # Default to 10 if not provided\n",
    "learning_rate = float(os.getenv(\"LEARNING_RATE\", 0.0001))  # Default to 0.001\n",
    "hidden_c = int(os.getenv(\"HIDDEN_C\", 1080))  # Default to 16\n",
    "random_seed = int(os.getenv(\"RANDOM_SEED\", 42))  # Default to 42\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "graph_num = os.getenv(\"GRAPH_NUM\", 11)\n",
    "dropout_p = float(os.getenv(\"DROPOUT\", 0.5))\n",
    "\n",
    "# wandb.login()\n",
    "# run = wandb.init(\n",
    "#     project=\"graph-embedding\",\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"hidden_c\": hidden_c,\n",
    "#         \"random_seed\": random_seed,\n",
    "#         \"num_layers\": num_layers,\n",
    "#         \"dropout_p\": dropout_p\n",
    "#     }\n",
    "# )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\", flush = True)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\", flush = True)\n",
    "\n",
    "### load graph data\n",
    "\n",
    "with open(f'../data/graphs/{graph_num}/linegraph_tg.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data.edge_index = data.edge_index.contiguous()\n",
    "data.x = data.x.contiguous()\n",
    "data.y = data.y.contiguous()\n",
    "\n",
    "sc = StandardScaler()\n",
    "data.x = torch.tensor(sc.fit_transform(data.x.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: my_autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=1080, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1080, out_features=540, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=540, out_features=270, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=270, out_features=540, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=540, out_features=1080, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1080, out_features=32, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianrasmussen/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = train_test_split(data.x, test_size=0.2, random_state=random_seed)\n",
    "train_loader = DataLoader(train_loader, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_loader, batch_size=64, shuffle=False)\n",
    "\n",
    "class my_autoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout_p):\n",
    "        super(my_autoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.Linear(hidden_channels//2, hidden_channels//4),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels//4, hidden_channels//2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.Linear(hidden_channels//2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.Linear(hidden_channels, in_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = my_autoencoder(data.x.shape[1], hidden_c, dropout_p).to(device)\n",
    "print(f\"Model: {model}\", flush = True)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.8872142193257493, Test Loss: 0.6687 current_lr : [0.0001]\n",
      "Epoch: 2, Train Loss: 0.6670925703155931, Test Loss: 0.4141 current_lr : [0.0001]\n",
      "Epoch: 3, Train Loss: 0.5290232642144753, Test Loss: 0.3082 current_lr : [0.0001]\n",
      "Epoch: 4, Train Loss: 0.44424400202654024, Test Loss: 0.2614 current_lr : [0.0001]\n",
      "Epoch: 5, Train Loss: 0.37384230410926556, Test Loss: 0.2297 current_lr : [0.0001]\n",
      "Epoch: 6, Train Loss: 0.3330358033260656, Test Loss: 0.2057 current_lr : [0.0001]\n",
      "Epoch: 7, Train Loss: 0.3078537588475873, Test Loss: 0.1879 current_lr : [0.0001]\n",
      "Epoch: 8, Train Loss: 0.2825813698114226, Test Loss: 0.1491 current_lr : [0.0001]\n",
      "Epoch: 9, Train Loss: 0.266774873944029, Test Loss: 0.1236 current_lr : [0.0001]\n",
      "Epoch: 10, Train Loss: 0.25665657019252497, Test Loss: 0.1032 current_lr : [0.0001]\n",
      "Epoch: 11, Train Loss: 0.23771619676518693, Test Loss: 0.1066 current_lr : [0.0001]\n",
      "Epoch: 12, Train Loss: 0.2298593233935732, Test Loss: 0.1049 current_lr : [0.0001]\n",
      "Epoch: 13, Train Loss: 0.2207810569199778, Test Loss: 0.0860 current_lr : [0.0001]\n",
      "Epoch: 14, Train Loss: 0.21805017016749217, Test Loss: 0.0918 current_lr : [0.0001]\n",
      "Epoch: 15, Train Loss: 0.2143686462844175, Test Loss: 0.0849 current_lr : [0.0001]\n",
      "Epoch: 16, Train Loss: 0.21454567484872997, Test Loss: 0.0863 current_lr : [0.0001]\n",
      "Epoch: 17, Train Loss: 0.20540856664615964, Test Loss: 0.0775 current_lr : [0.0001]\n",
      "Epoch: 18, Train Loss: 0.19990648112442128, Test Loss: 0.0866 current_lr : [0.0001]\n",
      "Epoch: 19, Train Loss: 0.19597746713688133, Test Loss: 0.0784 current_lr : [0.0001]\n",
      "Epoch: 20, Train Loss: 0.19355402229521326, Test Loss: 0.0712 current_lr : [0.0001]\n",
      "Epoch: 21, Train Loss: 0.1936345929230647, Test Loss: 0.0721 current_lr : [0.0001]\n",
      "Epoch: 22, Train Loss: 0.18710629738591336, Test Loss: 0.0766 current_lr : [0.0001]\n",
      "Epoch: 23, Train Loss: 0.19800724806608977, Test Loss: 0.0767 current_lr : [0.0001]\n",
      "Epoch: 24, Train Loss: 0.19736579062525558, Test Loss: 0.0775 current_lr : [0.0001]\n",
      "Epoch: 25, Train Loss: 0.17713167611016797, Test Loss: 0.0727 current_lr : [0.0001]\n",
      "Epoch: 26, Train Loss: 0.19008485561916752, Test Loss: 0.0774 current_lr : [0.0001]\n",
      "Epoch: 27, Train Loss: 0.18128990614540363, Test Loss: 0.0772 current_lr : [0.0001]\n",
      "Epoch: 28, Train Loss: 0.18426455955499063, Test Loss: 0.0691 current_lr : [0.0001]\n",
      "Epoch: 29, Train Loss: 0.17972124386677352, Test Loss: 0.0624 current_lr : [0.0001]\n",
      "Epoch: 30, Train Loss: 0.1763822256769768, Test Loss: 0.0686 current_lr : [0.0001]\n",
      "Epoch: 31, Train Loss: 0.17247504203841485, Test Loss: 0.0619 current_lr : [0.0001]\n",
      "Epoch: 32, Train Loss: 0.18214002196435575, Test Loss: 0.0613 current_lr : [0.0001]\n",
      "Epoch: 33, Train Loss: 0.16605068360844616, Test Loss: 0.0668 current_lr : [0.0001]\n",
      "Epoch: 34, Train Loss: 0.16923657352370874, Test Loss: 0.0579 current_lr : [0.0001]\n",
      "Epoch: 35, Train Loss: 0.17033396192170955, Test Loss: 0.0572 current_lr : [0.0001]\n",
      "Epoch: 36, Train Loss: 0.16516420539843027, Test Loss: 0.0684 current_lr : [0.0001]\n",
      "Epoch: 37, Train Loss: 0.16201929052276587, Test Loss: 0.0565 current_lr : [0.0001]\n",
      "Epoch: 38, Train Loss: 0.1652201217249351, Test Loss: 0.0713 current_lr : [0.0001]\n",
      "Epoch: 39, Train Loss: 0.16295144173794637, Test Loss: 0.0628 current_lr : [0.0001]\n",
      "Epoch: 40, Train Loss: 0.18172535834410203, Test Loss: 0.0782 current_lr : [0.0001]\n",
      "Epoch: 41, Train Loss: 0.16534792052374947, Test Loss: 0.0579 current_lr : [0.0001]\n",
      "Epoch: 42, Train Loss: 0.15415497984559762, Test Loss: 0.0627 current_lr : [0.0001]\n",
      "Epoch: 43, Train Loss: 0.16711519155946988, Test Loss: 0.0646 current_lr : [0.0001]\n",
      "Epoch: 44, Train Loss: 0.16332132318032483, Test Loss: 0.0584 current_lr : [0.0001]\n",
      "Epoch: 45, Train Loss: 0.1684100885811464, Test Loss: 0.0573 current_lr : [0.0001]\n",
      "Epoch: 46, Train Loss: 0.1498691237497109, Test Loss: 0.0590 current_lr : [0.0001]\n",
      "Epoch: 47, Train Loss: 0.15766659828405533, Test Loss: 0.0572 current_lr : [0.0001]\n",
      "Epoch: 48, Train Loss: 0.14716346941336436, Test Loss: 0.0515 current_lr : [0.0001]\n",
      "Epoch: 49, Train Loss: 0.15386810517382055, Test Loss: 0.0687 current_lr : [0.0001]\n",
      "Epoch: 50, Train Loss: 0.16752862291676657, Test Loss: 0.0479 current_lr : [0.0001]\n",
      "Epoch: 51, Train Loss: 0.15036385697583673, Test Loss: 0.0545 current_lr : [0.0001]\n",
      "Epoch: 52, Train Loss: 0.1525451935453232, Test Loss: 0.0496 current_lr : [0.0001]\n",
      "Epoch: 53, Train Loss: 0.14062939711388142, Test Loss: 0.0597 current_lr : [0.0001]\n",
      "Epoch: 54, Train Loss: 0.15070281051611775, Test Loss: 0.0491 current_lr : [0.0001]\n",
      "Epoch: 55, Train Loss: 0.14310644975020773, Test Loss: 0.0462 current_lr : [0.0001]\n",
      "Epoch: 56, Train Loss: 0.142448405060109, Test Loss: 0.0490 current_lr : [0.0001]\n",
      "Epoch: 57, Train Loss: 0.14665479032647988, Test Loss: 0.0617 current_lr : [0.0001]\n",
      "Epoch: 58, Train Loss: 0.1468954620409816, Test Loss: 0.0530 current_lr : [0.0001]\n",
      "Epoch: 59, Train Loss: 0.14219801960187772, Test Loss: 0.0467 current_lr : [0.0001]\n",
      "Epoch: 60, Train Loss: 0.1449837412724577, Test Loss: 0.0453 current_lr : [0.0001]\n",
      "Epoch: 61, Train Loss: 0.14894500851296086, Test Loss: 0.0482 current_lr : [0.0001]\n",
      "Epoch: 62, Train Loss: 0.13844608609126044, Test Loss: 0.0464 current_lr : [0.0001]\n",
      "Epoch: 63, Train Loss: 0.16416123574451794, Test Loss: 0.0496 current_lr : [0.0001]\n",
      "Epoch: 64, Train Loss: 0.14711665865723733, Test Loss: 0.0482 current_lr : [0.0001]\n",
      "Epoch: 65, Train Loss: 0.13819490928971578, Test Loss: 0.0506 current_lr : [0.0001]\n",
      "Epoch: 66, Train Loss: 0.14212988438232552, Test Loss: 0.0547 current_lr : [0.0001]\n",
      "Epoch: 67, Train Loss: 0.1406021330742136, Test Loss: 0.0463 current_lr : [0.0001]\n",
      "Epoch: 68, Train Loss: 0.15521488759035945, Test Loss: 0.0611 current_lr : [0.0001]\n",
      "Epoch: 69, Train Loss: 0.15063919101363768, Test Loss: 0.0525 current_lr : [0.0001]\n",
      "Epoch: 70, Train Loss: 0.14964199829945174, Test Loss: 0.0533 current_lr : [0.0001]\n",
      "Epoch: 71, Train Loss: 0.14001100436937083, Test Loss: 0.0465 current_lr : [0.0001]\n",
      "Epoch: 72, Train Loss: 0.13293387888186664, Test Loss: 0.0396 current_lr : [0.0001]\n",
      "Epoch: 73, Train Loss: 0.14258249344491453, Test Loss: 0.0542 current_lr : [0.0001]\n",
      "Epoch: 74, Train Loss: 0.1448266484562761, Test Loss: 0.0487 current_lr : [0.0001]\n",
      "Epoch: 75, Train Loss: 0.13739245318901286, Test Loss: 0.0452 current_lr : [0.0001]\n",
      "Epoch: 76, Train Loss: 0.1438368627377761, Test Loss: 0.0419 current_lr : [0.0001]\n",
      "Epoch: 77, Train Loss: 0.13451811198204283, Test Loss: 0.0489 current_lr : [0.0001]\n",
      "Epoch: 78, Train Loss: 0.14084280446841918, Test Loss: 0.0537 current_lr : [0.0001]\n",
      "Epoch: 79, Train Loss: 0.13349020471254353, Test Loss: 0.0430 current_lr : [0.0001]\n",
      "Epoch: 80, Train Loss: 0.138690028733835, Test Loss: 0.0544 current_lr : [0.0001]\n",
      "Epoch: 81, Train Loss: 0.1329184125704819, Test Loss: 0.0646 current_lr : [0.0001]\n",
      "Epoch: 82, Train Loss: 0.13198106819714503, Test Loss: 0.0489 current_lr : [0.0001]\n",
      "Epoch: 83, Train Loss: 0.13860515015268768, Test Loss: 0.0595 current_lr : [0.0001]\n",
      "Epoch: 84, Train Loss: 0.13792918672755597, Test Loss: 0.0401 current_lr : [0.0001]\n",
      "Epoch: 85, Train Loss: 0.1278048191396963, Test Loss: 0.0409 current_lr : [0.0001]\n",
      "Epoch: 86, Train Loss: 0.13421314674868154, Test Loss: 0.0463 current_lr : [0.0001]\n",
      "Epoch: 87, Train Loss: 0.13039533670735423, Test Loss: 0.0464 current_lr : [0.0001]\n",
      "Epoch: 88, Train Loss: 0.1372790657151392, Test Loss: 0.0529 current_lr : [0.0001]\n",
      "Epoch: 89, Train Loss: 0.13313434889944142, Test Loss: 0.0424 current_lr : [0.0001]\n",
      "Epoch: 90, Train Loss: 0.1421116523878284, Test Loss: 0.0428 current_lr : [0.0001]\n",
      "Epoch: 91, Train Loss: 0.13104552181841678, Test Loss: 0.0481 current_lr : [0.0001]\n",
      "Epoch: 92, Train Loss: 0.13937028756650982, Test Loss: 0.0518 current_lr : [0.0001]\n",
      "Epoch: 93, Train Loss: 0.1409441808672257, Test Loss: 0.0381 current_lr : [0.0001]\n",
      "Epoch: 94, Train Loss: 0.11912750342870673, Test Loss: 0.0429 current_lr : [0.0001]\n",
      "Epoch: 95, Train Loss: 0.1257642935961485, Test Loss: 0.0441 current_lr : [0.0001]\n",
      "Epoch: 96, Train Loss: 0.12418517662537476, Test Loss: 0.0409 current_lr : [0.0001]\n",
      "Epoch: 97, Train Loss: 0.12391223822025553, Test Loss: 0.0410 current_lr : [0.0001]\n",
      "Epoch: 98, Train Loss: 0.12323330290043953, Test Loss: 0.0506 current_lr : [0.0001]\n",
      "Epoch: 99, Train Loss: 0.12132423977175402, Test Loss: 0.0443 current_lr : [0.0001]\n",
      "Epoch: 100, Train Loss: 0.12289242847039113, Test Loss: 0.0398 current_lr : [0.0001]\n",
      "Epoch: 101, Train Loss: 0.12234947514084596, Test Loss: 0.0447 current_lr : [0.0001]\n",
      "Epoch: 102, Train Loss: 0.13496258645934403, Test Loss: 0.0422 current_lr : [0.0001]\n",
      "Epoch: 103, Train Loss: 0.12640618367081202, Test Loss: 0.0507 current_lr : [0.0001]\n",
      "Epoch: 104, Train Loss: 0.1328656120255353, Test Loss: 0.0436 current_lr : [0.0001]\n",
      "Epoch: 105, Train Loss: 0.13927427453614732, Test Loss: 0.0535 current_lr : [1e-05]\n",
      "Epoch: 106, Train Loss: 0.1371235405464494, Test Loss: 0.0492 current_lr : [1e-05]\n",
      "Epoch: 107, Train Loss: 0.13523494741490122, Test Loss: 0.0454 current_lr : [1e-05]\n",
      "Epoch: 108, Train Loss: 0.12241610620545332, Test Loss: 0.0433 current_lr : [1e-05]\n",
      "Epoch: 109, Train Loss: 0.12462753624650379, Test Loss: 0.0434 current_lr : [1e-05]\n",
      "Epoch: 110, Train Loss: 0.12507293126964694, Test Loss: 0.0404 current_lr : [1e-05]\n",
      "Epoch: 111, Train Loss: 0.12161175423257407, Test Loss: 0.0375 current_lr : [1e-05]\n",
      "Epoch: 112, Train Loss: 0.21524187982594842, Test Loss: 0.0379 current_lr : [1e-05]\n",
      "Epoch: 113, Train Loss: 0.11945671922522326, Test Loss: 0.0375 current_lr : [1e-05]\n",
      "Epoch: 114, Train Loss: 0.12100643658192542, Test Loss: 0.0358 current_lr : [1e-05]\n",
      "Epoch: 115, Train Loss: 0.11517533692457373, Test Loss: 0.0343 current_lr : [1e-05]\n",
      "Epoch: 116, Train Loss: 0.11724982949752341, Test Loss: 0.0342 current_lr : [1e-05]\n",
      "Epoch: 117, Train Loss: 0.11718606155503679, Test Loss: 0.0353 current_lr : [1e-05]\n",
      "Epoch: 118, Train Loss: 0.11178758466941496, Test Loss: 0.0371 current_lr : [1e-05]\n",
      "Epoch: 119, Train Loss: 0.1083364304667585, Test Loss: 0.0350 current_lr : [1e-05]\n",
      "Epoch: 120, Train Loss: 0.11670374481017312, Test Loss: 0.0345 current_lr : [1e-05]\n",
      "Epoch: 121, Train Loss: 0.11275935638203193, Test Loss: 0.0348 current_lr : [1e-05]\n",
      "Epoch: 122, Train Loss: 0.11525723589436403, Test Loss: 0.0360 current_lr : [1e-05]\n",
      "Epoch: 123, Train Loss: 0.11744587364848014, Test Loss: 0.0338 current_lr : [1e-05]\n",
      "Epoch: 124, Train Loss: 0.11368469464735537, Test Loss: 0.0338 current_lr : [1e-05]\n",
      "Epoch: 125, Train Loss: 0.1074534327077566, Test Loss: 0.0337 current_lr : [1e-05]\n",
      "Epoch: 126, Train Loss: 0.11217287502611283, Test Loss: 0.0336 current_lr : [1e-05]\n",
      "Epoch: 127, Train Loss: 0.11521692690809095, Test Loss: 0.0323 current_lr : [1e-05]\n",
      "Epoch: 128, Train Loss: 0.11140977926847953, Test Loss: 0.0327 current_lr : [1e-05]\n",
      "Epoch: 129, Train Loss: 0.10795296733538626, Test Loss: 0.0328 current_lr : [1e-05]\n",
      "Epoch: 130, Train Loss: 0.10630931762889737, Test Loss: 0.0338 current_lr : [1e-05]\n",
      "Epoch: 131, Train Loss: 0.11123404479905884, Test Loss: 0.0342 current_lr : [1e-05]\n",
      "Epoch: 132, Train Loss: 0.10943733731767645, Test Loss: 0.0319 current_lr : [1e-05]\n",
      "Epoch: 133, Train Loss: 0.11562584801798775, Test Loss: 0.0321 current_lr : [1e-05]\n",
      "Epoch: 134, Train Loss: 0.10188487250968893, Test Loss: 0.0330 current_lr : [1e-05]\n",
      "Epoch: 135, Train Loss: 0.1202767186172346, Test Loss: 0.0339 current_lr : [1e-05]\n",
      "Epoch: 136, Train Loss: 0.10930396148333789, Test Loss: 0.0324 current_lr : [1e-05]\n",
      "Epoch: 137, Train Loss: 0.1121083385848179, Test Loss: 0.0333 current_lr : [1e-05]\n",
      "Epoch: 138, Train Loss: 0.10780329685953874, Test Loss: 0.0314 current_lr : [1e-05]\n",
      "Epoch: 139, Train Loss: 0.10813059358212052, Test Loss: 0.0344 current_lr : [1e-05]\n",
      "Epoch: 140, Train Loss: 0.11628188522503016, Test Loss: 0.0338 current_lr : [1e-05]\n",
      "Epoch: 141, Train Loss: 0.1113658717798966, Test Loss: 0.0334 current_lr : [1e-05]\n",
      "Epoch: 142, Train Loss: 0.11063979245801137, Test Loss: 0.0309 current_lr : [1e-05]\n",
      "Epoch: 143, Train Loss: 0.1102394595564831, Test Loss: 0.0326 current_lr : [1e-05]\n",
      "Epoch: 144, Train Loss: 0.11177322526693975, Test Loss: 0.0328 current_lr : [1e-05]\n",
      "Epoch: 145, Train Loss: 0.11404162432958997, Test Loss: 0.0322 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 146, Train Loss: 0.11299244848094762, Test Loss: 0.0320 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 147, Train Loss: 0.11312457609172694, Test Loss: 0.0319 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 148, Train Loss: 0.1078505150274073, Test Loss: 0.0317 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 149, Train Loss: 0.11341749299219046, Test Loss: 0.0317 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 150, Train Loss: 0.11059728687599538, Test Loss: 0.0314 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 151, Train Loss: 0.10690734098867449, Test Loss: 0.0316 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 152, Train Loss: 0.1031925283993284, Test Loss: 0.0316 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 153, Train Loss: 0.103888640249217, Test Loss: 0.0313 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 154, Train Loss: 0.10819511250075367, Test Loss: 0.0312 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 155, Train Loss: 0.10998889222425759, Test Loss: 0.0312 current_lr : [1.0000000000000002e-06]\n",
      "Epoch: 156, Train Loss: 0.1066274832559641, Test Loss: 0.0310 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 157, Train Loss: 0.10734998327359636, Test Loss: 0.0310 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 158, Train Loss: 0.11084563190501834, Test Loss: 0.0310 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 159, Train Loss: 0.10303266788463271, Test Loss: 0.0310 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 160, Train Loss: 0.104961936465568, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 161, Train Loss: 0.10773205696531231, Test Loss: 0.0310 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 162, Train Loss: 0.10667770912762357, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 163, Train Loss: 0.11082164995451138, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 164, Train Loss: 0.11007668802800474, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 165, Train Loss: 0.1088032891786603, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 166, Train Loss: 0.10111553329363386, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 167, Train Loss: 0.10385667035484282, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 168, Train Loss: 0.11064174962501047, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 169, Train Loss: 0.10677167473646698, Test Loss: 0.0311 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 170, Train Loss: 0.10457479404334827, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 171, Train Loss: 0.10206785500936565, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 172, Train Loss: 0.11722101487968334, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 173, Train Loss: 0.10672202089890129, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 174, Train Loss: 0.11227206083103305, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 175, Train Loss: 0.10518690582029719, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 176, Train Loss: 0.10558704984073759, Test Loss: 0.0312 current_lr : [1.0000000000000002e-07]\n",
      "Epoch: 177, Train Loss: 0.1180819131029898, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 178, Train Loss: 0.10679748895326777, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 179, Train Loss: 0.10911776838952271, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 180, Train Loss: 0.1071800459176302, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 181, Train Loss: 0.10943275218997052, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 182, Train Loss: 0.10191449956603782, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 183, Train Loss: 0.10648686438798904, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 184, Train Loss: 0.10932253269606797, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 185, Train Loss: 0.10540891462375247, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 186, Train Loss: 0.11254621631214543, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 187, Train Loss: 0.1053847574367725, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 188, Train Loss: 0.10339964226026226, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 189, Train Loss: 0.11890052205749921, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 190, Train Loss: 0.1385530711560653, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 191, Train Loss: 0.1157769101706368, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 192, Train Loss: 0.12074521217968255, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 193, Train Loss: 0.10946767225802417, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 194, Train Loss: 0.10641995866699193, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 195, Train Loss: 0.11485291510406467, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 196, Train Loss: 0.10289641266205797, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 197, Train Loss: 0.11406632026450501, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 198, Train Loss: 0.11718671541739867, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 199, Train Loss: 0.10819082944678567, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 200, Train Loss: 0.12512038249483026, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 201, Train Loss: 0.10542909862108962, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 202, Train Loss: 0.11719161740666817, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 203, Train Loss: 0.10442587170040324, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 204, Train Loss: 0.11848814210425766, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 205, Train Loss: 0.11505997416972326, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 206, Train Loss: 0.10638897619136269, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 207, Train Loss: 0.10406100651909592, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 208, Train Loss: 0.10963185945101989, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 209, Train Loss: 0.10501988738203648, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 210, Train Loss: 0.1042820527341473, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 211, Train Loss: 0.10237832202344502, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 212, Train Loss: 0.112017487318664, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 213, Train Loss: 0.10453499689028062, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 214, Train Loss: 0.10794308667795526, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 215, Train Loss: 0.11307092911253373, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 216, Train Loss: 0.11399600465619375, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 217, Train Loss: 0.11621566684965892, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 218, Train Loss: 0.10165796535355705, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 219, Train Loss: 0.11120441430322235, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 220, Train Loss: 0.10685697548268806, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 221, Train Loss: 0.10613992681145352, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 222, Train Loss: 0.1073337521591278, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 223, Train Loss: 0.10925850137654278, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 224, Train Loss: 0.10818375549008133, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 225, Train Loss: 0.10609536471643619, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 226, Train Loss: 0.10513636705874609, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 227, Train Loss: 0.11540626556607624, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 228, Train Loss: 0.10592625939124634, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 229, Train Loss: 0.1004054442728086, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 230, Train Loss: 0.10609362070897112, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 231, Train Loss: 0.10871839692825994, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 232, Train Loss: 0.12010158397375591, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 233, Train Loss: 0.10314720099860872, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 234, Train Loss: 0.10319995897866431, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 235, Train Loss: 0.11262043054516195, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 236, Train Loss: 0.10412730680650505, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 237, Train Loss: 0.11010336437356219, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 238, Train Loss: 0.10840341135386437, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 239, Train Loss: 0.10580460057549533, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 240, Train Loss: 0.10316579928868032, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 241, Train Loss: 0.11245903301806677, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 242, Train Loss: 0.1228444574341651, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 243, Train Loss: 0.10776302285945763, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 244, Train Loss: 0.10929495554476504, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 245, Train Loss: 0.1081940015314748, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 246, Train Loss: 0.10418938079641925, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 247, Train Loss: 0.10627683028381653, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 248, Train Loss: 0.10942874091958242, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 249, Train Loss: 0.11627452892975675, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 250, Train Loss: 0.10741291923458299, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 251, Train Loss: 0.10282244107552937, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 252, Train Loss: 0.10582500157592001, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 253, Train Loss: 0.10705491121385306, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 254, Train Loss: 0.10926097725079488, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 255, Train Loss: 0.11511773726947251, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 256, Train Loss: 0.11219603753101731, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 257, Train Loss: 0.10064676153675589, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 258, Train Loss: 0.1110639995722859, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 259, Train Loss: 0.10626531361784569, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 260, Train Loss: 0.10549193708393624, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 261, Train Loss: 0.10941703822562303, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 262, Train Loss: 0.10379358873303447, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 263, Train Loss: 0.11149208981878858, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 264, Train Loss: 0.10567703056173823, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 265, Train Loss: 0.10835737947414004, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 266, Train Loss: 0.10775174593267145, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 267, Train Loss: 0.10571746073328156, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 268, Train Loss: 0.11214518257313305, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 269, Train Loss: 0.1058819135158229, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 270, Train Loss: 0.10987125273557409, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 271, Train Loss: 0.10546036681071633, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n",
      "Epoch: 272, Train Loss: 0.10667370204079561, Test Loss: 0.0312 current_lr : [1.0000000000000004e-08]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[1;32m     31\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n",
      "Cell \u001b[0;32mIn[83], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m out \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, batch)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    test_loss = test(test_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss:.4f} current_lr : {scheduler.get_last_lr()}', flush = True)\n",
    "print('Training complete', flush = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Splits data into train, validation, and test sets, stratifying by y > 0.\"\"\"\n",
    "\n",
    "    # Create a boolean mask for nodes where y > 0\n",
    "    positive_mask = data.y > 0\n",
    "\n",
    "    # Get indices of positive and negative nodes\n",
    "    positive_indices = positive_mask.nonzero(as_tuple=False).squeeze()\n",
    "    negative_indices = (~positive_mask).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    # Split positive indices\n",
    "    pos_train_idx, pos_temp_idx = train_test_split(positive_indices, train_size=train_ratio, random_state=random_seed)  # Adjust random_state for consistent splits\n",
    "    pos_val_idx, pos_test_idx = train_test_split(pos_temp_idx, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=random_seed)\n",
    "\n",
    "    # Split negative indices\n",
    "    neg_train_idx, neg_temp_idx = train_test_split(negative_indices, train_size=train_ratio, random_state=random_seed)\n",
    "    neg_val_idx, neg_test_idx = train_test_split(neg_temp_idx, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=random_seed)\n",
    "\n",
    "    # Combine indices\n",
    "    train_idx = torch.cat([pos_train_idx, neg_train_idx])\n",
    "    val_idx = torch.cat([pos_val_idx, neg_val_idx])\n",
    "    test_idx = torch.cat([pos_test_idx, neg_test_idx])\n",
    "\n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "data = stratified_split(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [int(i) for i in os.getenv(\"BINS\", \"3000\").split(' ')]  # Default to [1000, 3000, 5000]\n",
    "\n",
    "\n",
    "model.decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(hidden_c//4, hidden_c//2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(dropout_p),\n",
    "    torch.nn.Linear(hidden_c//2, hidden_c),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(dropout_p),\n",
    "    torch.nn.Linear(hidden_c, len(bins) + 1),\n",
    ")\n",
    "\n",
    "### freeze encoder\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = torch.tensor(bins, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    data.x = data.x.to(device)\n",
    "    data.edge_index = data.edge_index.to(device)\n",
    "    data.test_mask = data.test_mask.to(device)\n",
    "    data.y = data.y.to(device)\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    mask = data.train_mask.squeeze().to(device) & (data.y > 0).squeeze().to(device)\n",
    "    out = model(data.x)  # Perform a single forward pass.\n",
    "    # Convert target to 1D tensor with dtype=torch.long\n",
    "    target = torch.bucketize(data.y[mask], bins).squeeze()\n",
    "    loss = criterion(out[mask], target.long())  # Ensure target is 1D and long\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    data.x = data.x.to(device)\n",
    "    data.edge_index = data.edge_index.to(device)\n",
    "    data.val_mask = data.val_mask.to(device)\n",
    "    data.y = data.y.to(device)\n",
    "    mask = data.val_mask.squeeze() & (data.y > 0).squeeze()\n",
    "    out = model(data.x)\n",
    "    target = torch.bucketize(data.y[mask], bins).squeeze()\n",
    "    loss = criterion(out[mask], target.long())  # Ensure target is 1D and long\n",
    "    correct_preds = out[mask].argmax(dim=1)\n",
    "    correct = (correct_preds == target).sum()\n",
    "    accuracy = correct.item() / mask.sum().item()\n",
    "    return accuracy, out, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.7375357747077942, Val Loss: 0.6803, Val Accuracy: 0.5824\n",
      "Epoch: 2, Train Loss: 0.6905267238616943, Val Loss: 0.6685, Val Accuracy: 0.6264\n",
      "Epoch: 3, Train Loss: 0.6790911555290222, Val Loss: 0.6559, Val Accuracy: 0.6374\n",
      "Epoch: 4, Train Loss: 0.6679400205612183, Val Loss: 0.6386, Val Accuracy: 0.6703\n",
      "Epoch: 5, Train Loss: 0.6301417946815491, Val Loss: 0.6497, Val Accuracy: 0.6154\n",
      "Epoch: 6, Train Loss: 0.660032331943512, Val Loss: 0.6550, Val Accuracy: 0.6264\n",
      "Epoch: 7, Train Loss: 0.6483225226402283, Val Loss: 0.6436, Val Accuracy: 0.6703\n",
      "Epoch: 8, Train Loss: 0.6431363821029663, Val Loss: 0.6448, Val Accuracy: 0.6264\n",
      "Epoch: 9, Train Loss: 0.6053557991981506, Val Loss: 0.6525, Val Accuracy: 0.6593\n",
      "Epoch: 10, Train Loss: 0.6304991841316223, Val Loss: 0.6537, Val Accuracy: 0.6593\n",
      "Epoch: 11, Train Loss: 0.646774172782898, Val Loss: 0.6516, Val Accuracy: 0.6813\n",
      "Epoch: 12, Train Loss: 0.6159128546714783, Val Loss: 0.6509, Val Accuracy: 0.6154\n",
      "Epoch: 13, Train Loss: 0.5875295400619507, Val Loss: 0.6518, Val Accuracy: 0.6374\n",
      "Epoch: 14, Train Loss: 0.6057067513465881, Val Loss: 0.6487, Val Accuracy: 0.6703\n",
      "Epoch: 15, Train Loss: 0.5978676676750183, Val Loss: 0.6443, Val Accuracy: 0.6923\n",
      "Epoch: 16, Train Loss: 0.5916379690170288, Val Loss: 0.6431, Val Accuracy: 0.6813\n",
      "Epoch: 17, Train Loss: 0.5892230868339539, Val Loss: 0.6441, Val Accuracy: 0.6813\n",
      "Epoch: 18, Train Loss: 0.5837080478668213, Val Loss: 0.6450, Val Accuracy: 0.6703\n",
      "Epoch: 19, Train Loss: 0.5841194987297058, Val Loss: 0.6469, Val Accuracy: 0.6593\n",
      "Epoch: 20, Train Loss: 0.5643866658210754, Val Loss: 0.6519, Val Accuracy: 0.6703\n",
      "Epoch: 21, Train Loss: 0.5758472681045532, Val Loss: 0.6676, Val Accuracy: 0.6593\n",
      "Epoch: 22, Train Loss: 0.5839152336120605, Val Loss: 0.6861, Val Accuracy: 0.6593\n",
      "Epoch: 23, Train Loss: 0.5381465554237366, Val Loss: 0.7011, Val Accuracy: 0.6593\n",
      "Epoch: 24, Train Loss: 0.6029423475265503, Val Loss: 0.6985, Val Accuracy: 0.6593\n",
      "Epoch: 25, Train Loss: 0.5723719596862793, Val Loss: 0.6966, Val Accuracy: 0.6703\n",
      "Epoch: 26, Train Loss: 0.532899796962738, Val Loss: 0.6892, Val Accuracy: 0.6813\n",
      "Epoch: 27, Train Loss: 0.5622135400772095, Val Loss: 0.6801, Val Accuracy: 0.6703\n",
      "Epoch: 28, Train Loss: 0.5687844157218933, Val Loss: 0.6652, Val Accuracy: 0.6703\n",
      "Epoch: 29, Train Loss: 0.5713331699371338, Val Loss: 0.6564, Val Accuracy: 0.6703\n",
      "Epoch: 30, Train Loss: 0.5469558238983154, Val Loss: 0.6501, Val Accuracy: 0.6703\n",
      "Epoch: 31, Train Loss: 0.5650244951248169, Val Loss: 0.6461, Val Accuracy: 0.6703\n",
      "Epoch: 32, Train Loss: 0.542282223701477, Val Loss: 0.6456, Val Accuracy: 0.6813\n",
      "Epoch: 33, Train Loss: 0.5414911508560181, Val Loss: 0.6495, Val Accuracy: 0.6923\n",
      "Epoch: 34, Train Loss: 0.5406589508056641, Val Loss: 0.6497, Val Accuracy: 0.6923\n",
      "Epoch: 35, Train Loss: 0.5614851117134094, Val Loss: 0.6516, Val Accuracy: 0.6813\n",
      "Epoch: 36, Train Loss: 0.5245265364646912, Val Loss: 0.6548, Val Accuracy: 0.6703\n",
      "Epoch: 37, Train Loss: 0.54104083776474, Val Loss: 0.6601, Val Accuracy: 0.6703\n",
      "Epoch: 38, Train Loss: 0.5562599897384644, Val Loss: 0.6577, Val Accuracy: 0.6813\n",
      "Epoch: 39, Train Loss: 0.5347179174423218, Val Loss: 0.6588, Val Accuracy: 0.6923\n",
      "Epoch: 40, Train Loss: 0.5353506207466125, Val Loss: 0.6620, Val Accuracy: 0.6813\n",
      "Epoch: 41, Train Loss: 0.5245749950408936, Val Loss: 0.6647, Val Accuracy: 0.6813\n",
      "Epoch: 42, Train Loss: 0.5146124958992004, Val Loss: 0.6664, Val Accuracy: 0.6813\n",
      "Epoch: 43, Train Loss: 0.5482942461967468, Val Loss: 0.6679, Val Accuracy: 0.6923\n",
      "Epoch: 44, Train Loss: 0.5272610783576965, Val Loss: 0.6707, Val Accuracy: 0.6923\n",
      "Epoch: 45, Train Loss: 0.5319187641143799, Val Loss: 0.6769, Val Accuracy: 0.6593\n",
      "Epoch: 46, Train Loss: 0.5211263298988342, Val Loss: 0.6789, Val Accuracy: 0.6593\n",
      "Epoch: 47, Train Loss: 0.5326030850410461, Val Loss: 0.6749, Val Accuracy: 0.6703\n",
      "Epoch: 48, Train Loss: 0.5245702862739563, Val Loss: 0.6721, Val Accuracy: 0.6703\n",
      "Epoch: 49, Train Loss: 0.5154688954353333, Val Loss: 0.6703, Val Accuracy: 0.6923\n",
      "Epoch: 50, Train Loss: 0.4861791431903839, Val Loss: 0.6718, Val Accuracy: 0.6813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     val_accuracy, out, val_loss \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m      4\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n",
      "Cell \u001b[0;32mIn[99], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m mask \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtrain_mask\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m&\u001b[39m (data\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert target to 1D tensor with dtype=torch.long\u001b[39;00m\n\u001b[1;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbucketize(data\u001b[38;5;241m.\u001b[39my[mask], bins)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[82], line 28\u001b[0m, in \u001b[0;36mmy_autoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    val_accuracy, out, val_loss = test()\n",
    "    scheduler.step(train_loss)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}', flush = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
