{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1303a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "from torch_geometric.nn.models import Node2Vec\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "GCNConv._orig_propagate = GCNConv.propagate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.explain import GNNExplainer, Explainer\n",
    "\n",
    "\n",
    "epochs = int(os.getenv(\"EPOCHS\", 10))  # Default to 10 if not provided\n",
    "learning_rate = float(os.getenv(\"LEARNING_RATE\", 0.001))  # Default to 0.001\n",
    "hidden_c = int(os.getenv(\"HIDDEN_C\", 16))  # Default to 16\n",
    "random_seed = int(os.getenv(\"RANDOM_SEED\", 42))  # Default to 42\n",
    "bins = [int(i) for i in os.getenv(\"BINS\", \"400 800 1300 2100 3000 3700 4700 7020 9660\").split(' ')]  # Default to [1000, 3000, 5000]\n",
    "bins = [int(i) for i in os.getenv(\"BINS\", \"3000\").split(' ')]  # Default to [1000, 3000, 5000]\n",
    "num_layers = int(os.getenv(\"NUM_LAYERS\", 5))  # Default to 5\n",
    "nh = int(os.getenv(\"NUM_HEADS\", 10))\n",
    "gat = int(os.getenv(\"GAT\", 0))\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "graph_num = os.getenv(\"GRAPH_NUM\", 2)\n",
    "dropout_p = float(os.getenv(\"DROPOUT\", 0.5))\n",
    "\n",
    "bins = torch.tensor(bins, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "graph_num = 20\n",
    "\n",
    "model_name = 'woven-yogurt-280'  # Replace with your model name\n",
    "weight_prefix = 'best_accuracy'  # Replace with your weight prefix\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\", flush = True)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\", flush = True)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "with open(f'../data/graphs/{graph_num}/linegraph_tg.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data.edge_index = data.edge_index.contiguous()\n",
    "data.x = data.x.contiguous()\n",
    "data.y = data.y.contiguous()\n",
    "\n",
    "# Define or import the GCN class\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        self.conv1 = GCNConv(data.num_features, hidden_channels, improved = True, cached = True)\n",
    "        conv2_list = []\n",
    "        hc = hidden_channels\n",
    "        # for _ in range(num_layers):\n",
    "        #     conv2_list.append(\n",
    "        #         GCNConv(hc, hc)\n",
    "        #     )\n",
    "            # hc //= 2\n",
    "        # self.conv2 = torch.nn.ModuleList(conv2_list)\n",
    "        self.conv3 = GCNConv(hc, len(bins) + 1, cached = True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=dropout_p, training=self.training)\n",
    "        # for conv in self.conv2:\n",
    "        #     x = conv(x, edge_index)\n",
    "        #     x = F.relu(x)\n",
    "        #     x = F.dropout(x, p=dropout_p, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Load the model with the GCN class\n",
    "model = torch.load(f'../data/graphs/{graph_num}/models/{model_name}.pt', map_location=device)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../data/graphs/{graph_num}/models/{model_name}_{weight_prefix}.pt', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Splits data into train, validation, and test sets, stratifying by y > 0.\"\"\"\n",
    "\n",
    "    # Create a boolean mask for nodes where y > 0\n",
    "    positive_mask = data.y > 0\n",
    "\n",
    "    # Get indices of positive and negative nodes\n",
    "    positive_indices = positive_mask.nonzero(as_tuple=False).squeeze()\n",
    "    negative_indices = (~positive_mask).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    # Split positive indices\n",
    "    pos_train_idx, pos_temp_idx = train_test_split(positive_indices, train_size=train_ratio, random_state=random_seed)  # Adjust random_state for consistent splits\n",
    "    pos_val_idx, pos_test_idx = train_test_split(pos_temp_idx, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=random_seed)\n",
    "\n",
    "    # Split negative indices\n",
    "    neg_train_idx, neg_temp_idx = train_test_split(negative_indices, train_size=train_ratio, random_state=random_seed)\n",
    "    neg_val_idx, neg_test_idx = train_test_split(neg_temp_idx, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=random_seed)\n",
    "\n",
    "    # Combine indices\n",
    "    train_idx = torch.cat([pos_train_idx, neg_train_idx])\n",
    "    val_idx = torch.cat([pos_val_idx, neg_val_idx])\n",
    "    test_idx = torch.cat([pos_test_idx, neg_test_idx])\n",
    "\n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "data.edge_index = data.edge_index.contiguous()\n",
    "data.x = data.x.contiguous()\n",
    "data.y = data.y.contiguous()\n",
    "\n",
    "print(data.x.shape, data.edge_index.shape, data.y.shape, flush = True)\n",
    "\n",
    "data = stratified_split(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import GNNExplainer, Explainer\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=1),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type=None,\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data.val_mask.squeeze() & (data.y > 0).squeeze()\n",
    "\n",
    "node_idx = torch.nonzero(mask, as_tuple=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data.x.to(device), data.edge_index.to(device))\n",
    "pred = out.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d503ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_feats = []\n",
    "scores = {}\n",
    "gradients = {}  # <- new dict for feature gradients (signed influence)\n",
    "\n",
    "data_x = data.x.to(device)\n",
    "edge_index = data.edge_index.to(device)\n",
    "data_y = data.y.to(device)\n",
    "\n",
    "data_x.requires_grad_(True)  # Enable gradients w.r.t. input features\n",
    "\n",
    "for idx in node_idx:\n",
    "    # 1. Get explanation (feature importance mask)\n",
    "    explanation = explainer(data_x, edge_index, index=idx)\n",
    "    node_mask = explanation.node_mask.squeeze()\n",
    "\n",
    "    # 2. Get prediction and target\n",
    "    curr_pred = pred[idx].item()\n",
    "    target = int(torch.bucketize(data_y[idx].to(bins.device), bins))\n",
    "\n",
    "    # 3. Sum across nodes (should be 1 node) and flatten\n",
    "    score = node_mask.sum(dim=0).detach().cpu().numpy().flatten()\n",
    "\n",
    "    # 4. Compute gradients w.r.t. current node\n",
    "    model.zero_grad()\n",
    "    out = model(data_x, edge_index)\n",
    "    out[idx].max().backward(retain_graph=True)  # Backprop from top prediction score\n",
    "    node_grad = data_x.grad[idx].detach().cpu().numpy().flatten()\n",
    "\n",
    "    # 5. Top features by importance\n",
    "    top10_idx = np.argsort(score)[::-1][:10]\n",
    "    top10_scores = score[top10_idx]\n",
    "    top10_gradients = node_grad[top10_idx]\n",
    "\n",
    "    print(\n",
    "        f\"Top features for node {idx}:\\n\",\n",
    "        list(zip(top10_idx[top10_scores > 0], top10_scores[top10_scores > 0])),\n",
    "        \"\\nGradients (signed influence):\\n\",\n",
    "        list(zip(top10_idx[top10_scores > 0], top10_gradients[top10_scores > 0])),\n",
    "        f\"\\nPredicted class: {curr_pred}, actual class: {target}\",\n",
    "        \"\\n---------------------------------------\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    # 6. Save results\n",
    "    for feat_idx in top10_idx:\n",
    "        used_feats.append(feat_idx)\n",
    "        if feat_idx not in scores:\n",
    "            scores[feat_idx] = []\n",
    "            gradients[feat_idx] = []\n",
    "        scores[feat_idx].append(score[feat_idx])\n",
    "        gradients[feat_idx].append(node_grad[feat_idx])\n",
    "\n",
    "    # Clear gradients for next iteration\n",
    "    data_x.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a409e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {k: np.mean(v) for k, v in gradients.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc902244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert values to float (round to 3 decimal places)\n",
    "for k, v in gradients.items():\n",
    "    gradients[k] = round(float(v), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c93cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feats_df = pd.read_csv(f'../data/graphs/{graph_num}/node_features.csv')\n",
    "if 'aadt' in feats_df.columns:\n",
    "    feats_df = feats_df.drop(columns=['aadt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f10a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort  gradients by key\n",
    "sorted_gradients = dict(sorted(gradients.items(), key=lambda item: item[0]))\n",
    "\n",
    "# replace keys with df column names\n",
    "sorted_gradients = {feats_df.columns[k]: v for k, v in sorted_gradients.items()}\n",
    "# convert to dataframe\n",
    "sorted_gradients_df = pd.DataFrame(sorted_gradients.items(), columns=['Feature', 'Gradient'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_gradients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Step 1: Define parameters\n",
    "lat, lon = 55.6867243, 12.5700724\n",
    "dist = 10000  # in meters\n",
    "features = [\n",
    "    'amenity', 'shop', 'building', 'aerialway', 'aeroway',\n",
    "    'barrier', 'boundary', 'craft', 'emergency', 'highway',\n",
    "    'historic', 'landuse', 'leisure', 'healthcare', 'military',\n",
    "    'natural', 'office', 'power', 'public_transport', 'railway',\n",
    "    'place', 'service', 'tourism', 'waterway', 'route', 'water'\n",
    "]\n",
    "\n",
    "# Step 2: Download features\n",
    "tags = {feat: True for feat in features}\n",
    "print(\"ðŸ“¥ Downloading OSM features...\")\n",
    "amenities = ox.features.features_from_point((lat, lon), tags=tags, dist=dist)\n",
    "\n",
    "print(f\"âœ… Downloaded {len(amenities)} OSM features.\")\n",
    "\n",
    "# Step 3: Build value-to-column dictionary\n",
    "value_to_column = {}\n",
    "\n",
    "for feature in features:\n",
    "    if feature in amenities.columns:\n",
    "        unique_values = amenities[feature].dropna().unique()\n",
    "        for value in unique_values:\n",
    "            value = str(value).strip()\n",
    "            if value and value.lower() != 'nan':\n",
    "                value_to_column[value] = feature\n",
    "\n",
    "print(f\"âœ… Collected {len(value_to_column)} value-to-column pairs.\")\n",
    "\n",
    "# Step 4: Save to file\n",
    "with open('osm_value_to_column.json', 'w') as f:\n",
    "    json.dump(value_to_column, f, indent=2)\n",
    "\n",
    "print(\"âœ… Saved value-to-column dictionary to 'osm_value_to_column.json'\")\n",
    "\n",
    "features = [\n",
    "    'amenity', 'shop', 'building', 'aerialway', 'aeroway',\n",
    "    'barrier', 'boundary', 'craft', 'emergency', 'highway',\n",
    "    'historic', 'landuse', 'leisure', 'healthcare', 'military',\n",
    "    'natural', 'office', 'power', 'public_transport', 'railway',\n",
    "    'place', 'service', 'tourism', 'waterway', 'route', 'water'\n",
    "]\n",
    "\n",
    "# Step 2: Download features\n",
    "tags = {feat: True for feat in features}\n",
    "print(\"ðŸ“¥ Downloading OSM features...\")\n",
    "amenities = ox.features.features_from_point((lat, lon), tags=tags, dist=dist)\n",
    "\n",
    "print(f\"âœ… Downloaded {len(amenities)} OSM features.\")\n",
    "\n",
    "# Step 3: Build value-to-column dictionary\n",
    "value_to_column = {}\n",
    "\n",
    "for feature in features:\n",
    "    if feature in amenities.columns:\n",
    "        unique_values = amenities[feature].dropna().unique()\n",
    "        for value in unique_values:\n",
    "            value = str(value).strip()\n",
    "            if value and value.lower() != 'nan':\n",
    "                value_to_column[value] = feature\n",
    "\n",
    "print(f\"âœ… Collected {len(value_to_column)} value-to-column pairs.\")\n",
    "\n",
    "# Step 4: Save to file\n",
    "with open('osm_value_to_column.json', 'w') as f:\n",
    "    json.dump(value_to_column, f, indent=2)\n",
    "\n",
    "print(\"âœ… Saved value-to-column dictionary to 'osm_value_to_column.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume 'graph_num', 'node_idx', 'data', 'model', 'explainer' already exist\n",
    "\n",
    "# Load feature names\n",
    "feats_df = pd.read_csv(f'../data/graphs/{graph_num}/node_features.csv')\n",
    "if 'aadt' in feats_df.columns:\n",
    "    feats_df = feats_df.drop(columns=['aadt'])\n",
    "\n",
    "# Your secondary to primary mapping\n",
    "secondary_to_primary = {\n",
    "    # [your huge dictionary goes here... I won't paste it all again here]\n",
    "}\n",
    "\n",
    "used_feats = []\n",
    "scores = {}\n",
    "gradients = {}  # Per-node average gradient collection\n",
    "all_bin_grads_total = {}  # Collect per-bin gradients\n",
    "\n",
    "data_x = data.x.to(device)\n",
    "edge_index = data.edge_index.to(device)\n",
    "data_y = data.y.to(device)\n",
    "\n",
    "data_x.requires_grad_(True)\n",
    "\n",
    "for idx in node_idx:\n",
    "    # 1. Explain the node\n",
    "    explanation = explainer(data_x, edge_index, index=idx)\n",
    "    node_mask = explanation.node_mask.squeeze()\n",
    "\n",
    "    # 2. Prediction and true label\n",
    "    curr_pred = pred[idx].item()\n",
    "    target = int(torch.bucketize(data_y[idx], bins.to(data_y.device)))\n",
    "\n",
    "    # 3. Importance scores (feature mask)\n",
    "    score = node_mask.sum(dim=0).detach().cpu().numpy().flatten()\n",
    "\n",
    "    # 4. Per-bin gradients\n",
    "    node_bin_grads = []\n",
    "    for bin_idx in range(model(data_x, edge_index).shape[1]):\n",
    "        model.zero_grad()\n",
    "        out = model(data_x, edge_index)\n",
    "        out[idx, bin_idx].backward(retain_graph=True)\n",
    "        grad = data_x.grad[idx].detach().cpu().numpy().flatten()\n",
    "        node_bin_grads.append(grad)\n",
    "        data_x.grad.zero_()\n",
    "    node_bin_grads = np.stack(node_bin_grads)  # (num_bins, num_features)\n",
    "\n",
    "    # 5. Save gradients per feature\n",
    "    for feat_idx in range(node_bin_grads.shape[1]):\n",
    "        if feat_idx not in all_bin_grads_total:\n",
    "            all_bin_grads_total[feat_idx] = []\n",
    "        all_bin_grads_total[feat_idx].append(node_bin_grads[:, feat_idx])\n",
    "\n",
    "    # 6. Top 10 features\n",
    "    top10_idx = np.argsort(score)[::-1][:10]\n",
    "    top10_scores = score[top10_idx]\n",
    "\n",
    "    # 7. Collect average gradients\n",
    "    for feat_idx in top10_idx:\n",
    "        used_feats.append(feat_idx)\n",
    "        if feat_idx not in scores:\n",
    "            scores[feat_idx] = []\n",
    "            gradients[feat_idx] = []\n",
    "        scores[feat_idx].append(score[feat_idx])\n",
    "        gradients[feat_idx].append(node_bin_grads[target, feat_idx])  # (you can choose to collect gradients differently)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85727ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df866c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# After all nodes are explained\n",
    "# ================================\n",
    "\n",
    "# Sort by feature index\n",
    "sorted_gradients = dict(sorted(gradients.items(), key=lambda item: item[0]))\n",
    "\n",
    "# Map feature index â†’ feature name\n",
    "sorted_gradients_named = {feats_df.columns[k]: v for k, v in sorted_gradients.items()}\n",
    "\n",
    "# Average importance score\n",
    "average_gradients = {feat: np.mean(values) for feat, values in sorted_gradients_named.items()}\n",
    "\n",
    "# =====================================\n",
    "# Fit real Linear Regression (gradients vs bin index)\n",
    "# =====================================\n",
    "\n",
    "feature_slopes = {}\n",
    "bin_indices = np.arange(len(bins)+1).reshape(-1, 1)  # 10 bins (0-9)\n",
    "\n",
    "for feat_idx, feat_name in enumerate(feats_df.columns):\n",
    "    if feat_idx not in all_bin_grads_total:\n",
    "        continue\n",
    "\n",
    "    # Stack across all explained nodes\n",
    "    grads_across_nodes = np.stack(all_bin_grads_total[feat_idx])  # (num_nodes, num_bins)\n",
    "    avg_grads_across_bins = np.mean(grads_across_nodes, axis=0)\n",
    "\n",
    "    reg = LinearRegression().fit(bin_indices, avg_grads_across_bins)\n",
    "    feature_slopes[feat_name] = reg.coef_[0]\n",
    "\n",
    "# =====================================\n",
    "# Build final DataFrame\n",
    "# =====================================\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Feature': list(average_gradients.keys()),\n",
    "    'Avg_Gradient': list(average_gradients.values()),\n",
    "    'Gradient_Trend_Slope': [feature_slopes.get(feat, 0.0) for feat in average_gradients.keys()]\n",
    "})\n",
    "\n",
    "summary_df['Abs_Avg_Gradient'] = summary_df['Avg_Gradient'].abs()\n",
    "\n",
    "# ðŸ§  Group features based on secondary_to_primary\n",
    "def map_primary(feature_name):\n",
    "    return value_to_column.get(feature_name, 'Other')  # default to 'Other' if not found\n",
    "\n",
    "summary_df['Primary_Category'] = summary_df['Feature'].apply(map_primary)\n",
    "\n",
    "# Sort by Primary Category then by Absolute Gradient\n",
    "summary_df = summary_df.sort_values(['Primary_Category', 'Abs_Avg_Gradient'], ascending=[True, False])\n",
    "\n",
    "# Save final output\n",
    "summary_df.to_csv('feature_gradients_grouped_by_primary.csv', index=False)\n",
    "\n",
    "print(\"âœ… Saved grouped feature summary to 'feature_gradients_grouped_by_primary.csv'\")\n",
    "print(summary_df.head(15))  # Show first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ecc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df.sort_values('Gradient_Trend_Slope', ascending=False).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cff411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ§  Step 1: Build grouped feature Ã— bin matrix using summary_df\n",
    "\n",
    "feature_names = list(summary_df['Feature'])\n",
    "primary_categories = list(summary_df['Primary_Category'])\n",
    "\n",
    "# Match size to bins + 1\n",
    "feature_bin_gradients = np.zeros((len(feature_names), len(bins) + 1))\n",
    "\n",
    "for idx, feat_name in enumerate(feature_names):\n",
    "    feat_idx_in_feats_df = list(feats_df.columns).index(feat_name)\n",
    "    if feat_idx_in_feats_df not in all_bin_grads_total:\n",
    "        continue\n",
    "\n",
    "    grads_across_nodes = np.stack(all_bin_grads_total[feat_idx_in_feats_df])  # (num_nodes x num_bins)\n",
    "    avg_grads_per_bin = np.mean(grads_across_nodes, axis=0)  # (num_bins,)\n",
    "    feature_bin_gradients[idx, :] = avg_grads_per_bin\n",
    "\n",
    "# ðŸ§  Step 2: Compute Spearman correlation per feature\n",
    "\n",
    "spearman_corrs = []\n",
    "\n",
    "for idx in range(feature_bin_gradients.shape[0]):\n",
    "    grads = feature_bin_gradients[idx, :]\n",
    "    bin_indices = np.arange(len(bins) + 1)\n",
    "\n",
    "    if np.all(grads == 0):\n",
    "        spearman_corr = 0.0\n",
    "    else:\n",
    "        spearman_corr, _ = spearmanr(bin_indices, grads)\n",
    "\n",
    "    spearman_corrs.append(spearman_corr)\n",
    "\n",
    "spearman_corrs = np.array(spearman_corrs)\n",
    "\n",
    "# ðŸ§  Step 3: Prepare for plotting (grouped by Primary Category)\n",
    "\n",
    "# Build DataFrame to help sorting/grouping\n",
    "plot_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Primary_Category': primary_categories,\n",
    "    'Spearman_Corr': spearman_corrs\n",
    "})\n",
    "\n",
    "# Sort features by Primary Category first, then by absolute Spearman correlation\n",
    "plot_df = plot_df.sort_values(['Primary_Category', 'Spearman_Corr'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Reorder matrices according to sorted features\n",
    "sorted_feature_bin_gradients = feature_bin_gradients[plot_df.index, :]\n",
    "sorted_spearman_corrs = plot_df['Spearman_Corr'].values\n",
    "sorted_feature_names = plot_df['Feature'].values\n",
    "sorted_primary_categories = plot_df['Primary_Category'].values\n",
    "\n",
    "# ðŸ§  Step 4: Plot side-by-side\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, len(sorted_feature_names) * 0.3))\n",
    "\n",
    "# ðŸ”¥ Left: Gradients heatmap\n",
    "sns.heatmap(\n",
    "    sorted_feature_bin_gradients,\n",
    "    ax=axes[0],\n",
    "    xticklabels=[f\"Bin {i}\" for i in range(len(bins)+1)],\n",
    "    yticklabels=[f\"{cat} - {feat}\" for cat, feat in zip(sorted_primary_categories, sorted_feature_names)],\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"label\": \"Gradient Value\"}\n",
    ")\n",
    "axes[0].set_title(\"Feature Gradients Across Bins (Grouped)\", fontsize=16)\n",
    "axes[0].set_xlabel(\"Bin Index\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Feature (Grouped)\", fontsize=14)\n",
    "\n",
    "# ðŸ”¥ Right: Spearman correlation heatmap\n",
    "sns.heatmap(\n",
    "    sorted_spearman_corrs.reshape(-1, 1),\n",
    "    ax=axes[1],\n",
    "    yticklabels=[f\"{cat} - {feat}\" for cat, feat in zip(sorted_primary_categories, sorted_feature_names)],\n",
    "    xticklabels=[\"Spearman Correlation\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"label\": \"Correlation\"}\n",
    ")\n",
    "axes[1].set_title(\"Spearman Correlation (Gradient vs Bin)\", fontsize=16)\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3babe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ§  Step 1: Prepare features and y\n",
    "\n",
    "# Load feature data\n",
    "X = pd.read_csv(f'../data/graphs/{graph_num}/node_features.csv')\n",
    "if 'aadt' in X.columns:\n",
    "    X = X.drop(columns=['aadt'])\n",
    "y = data.y.cpu().numpy()  # Your original target values (unbinned)\n",
    "\n",
    "# ðŸ§  Step 2: Mask X and y where y > 0\n",
    "\n",
    "mask = y > 0\n",
    "X_filtered = X.iloc[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# ðŸ§  Step 3: Compute Spearman correlation for each feature vs. y_filtered\n",
    "\n",
    "base_spearman_corrs = {}\n",
    "\n",
    "for feat in X_filtered.columns:\n",
    "    feature_values = X_filtered[feat].values\n",
    "\n",
    "    # Handle constant features\n",
    "    if np.all(feature_values == feature_values[0]):\n",
    "        corr = 0.0\n",
    "    else:\n",
    "        corr, _ = spearmanr(feature_values, y_filtered)\n",
    "\n",
    "    base_spearman_corrs[feat] = corr\n",
    "\n",
    "# ðŸ§  Step 4: Convert to DataFrame for inspection\n",
    "\n",
    "base_corr_df = pd.DataFrame({\n",
    "    'Feature': list(base_spearman_corrs.keys()),\n",
    "    'Spearman_Feature_vs_Target': list(base_spearman_corrs.values())\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ðŸ“œ Assume `base_corr_df` is already created from previous code\n",
    "# (if not, rerun the cell that computes base Spearman correlations)\n",
    "\n",
    "# Sort features by absolute Spearman correlation (strongest ones first)\n",
    "base_corr_df['Abs_Spearman'] = base_corr_df['Spearman_Feature_vs_Target'].abs()\n",
    "base_corr_df = base_corr_df.sort_values('Abs_Spearman', ascending=False)\n",
    "\n",
    "# ðŸ”¥ Plot\n",
    "plt.figure(figsize=(12, len(base_corr_df) * 0.3))  # Auto-scale height\n",
    "\n",
    "sns.barplot(\n",
    "    data=base_corr_df,\n",
    "    y='Feature',\n",
    "    x='Spearman_Feature_vs_Target',\n",
    "    palette='coolwarm'\n",
    ")\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='--')  # Add vertical line at 0\n",
    "plt.title('Base Spearman Correlation (Feature vs Target)', fontsize=16)\n",
    "plt.xlabel('Spearman Correlation', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_feats = []\n",
    "# scores = {}\n",
    "\n",
    "\n",
    "# for i in node_idx:\n",
    "#     # Input data must include x and edge_index, and optionally y\n",
    "#     explanation = explainer(data.x.to(device), data.edge_index.to(device), index=i)\n",
    "#     # Ensure node_mask is 2D\n",
    "#     node_mask = explanation.node_mask\n",
    "#     if node_mask.dim() == 1:\n",
    "#         node_mask = node_mask.unsqueeze(0)\n",
    "#     elif node_mask.dim() == 3:\n",
    "#         node_mask = node_mask.squeeze(0)\n",
    "    \n",
    "\n",
    "#     curr_pred = pred[i].item()\n",
    "#     target = data.y[i].item()\n",
    "#     target = int(torch.bucketize(target, bins))\n",
    "\n",
    "#     # Sum across nodes (or use first if only one node)\n",
    "#     score = node_mask.sum(dim=0).detach().cpu().numpy()\n",
    "#     score = score.flatten()  # ensure 1D\n",
    "\n",
    "#     # Ensure labels are native Python list (not np array or tensor)\n",
    "#     feat_labels = [f\"feat_{i}\" for i in range(score.shape[0])]\n",
    "#     top10 = np.argsort(score)[::-1][:10]  # Get indices of top 10 features\n",
    "#     top10_score = score[top10]  # Get scores of top 10 features\n",
    "#     print(top10[top10_score > 0], top10_score[top10_score > 0], '\\n PRedicted class:' ,curr_pred, 'actual class:', target, '\\n---------------------------------------', flush = True)\n",
    "#     for i in top10:\n",
    "#         used_feats.append(i)\n",
    "#         if i not in scores:\n",
    "#             scores[i] = []\n",
    "#         scores[i].append(score[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(used_feats).most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56db399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in scores:\n",
    "#     scores[i] = np.mean(scores[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670308e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
